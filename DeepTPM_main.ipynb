{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a46665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from argparse import ArgumentParser\n",
    "import random\n",
    "import torch\n",
    "from torch.nn import init\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score,roc_curve, auc\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import scipy.io\n",
    "import csv\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0561954-b16c-40e9-a7a7-59195dd35531",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_hla_dataframe(fname):\n",
    "    df = pd.read_table(fname, skiprows=[0], sep=',', names=['CDR3', 'Antigen', 'HLA', 'binder', 'MHC'])\n",
    "    return df\n",
    "\n",
    "def compute_metrics1(y_labels, y_preds, cutoff=0.5, digit=3):\n",
    "    # If a valid threshold is not specified, use Youden's J statistic to choose the optimal threshold\n",
    "    if (cutoff <= 0) | (cutoff >= 1):\n",
    "        fpr, tpr, threshold = roc_curve(y_labels, y_preds)\n",
    "        cutoff = sorted(list(zip(np.abs(tpr - fpr), threshold)), key=lambda i: i[0], reverse=True)[0][1]\n",
    "        pass\n",
    "    \n",
    "    y_pred_labels = np.array([1. if p >= cutoff else 0. for p in y_preds])\n",
    "    Accuracy = accuracy_score(y_labels, y_pred_labels) # Accuracy\n",
    "    # Precision = precision_score(y_labels, y_pred_labels) # Precision\n",
    "    Recall = recall_score(y_labels, y_pred_labels) # Recall (Sensitivity)\n",
    "    Specificity = recall_score(1 - y_labels, 1 - y_pred_labels) # Specificity\n",
    "    F1 = f1_score(y_labels, y_pred_labels) # F1 score\n",
    "    AUC = roc_auc_score(y_labels, y_preds) # AUC\n",
    "    \n",
    "    return {\n",
    "        'AUC':         np.round(AUC,         digit),\n",
    "        'Accuracy':    np.round(Accuracy,    digit),\n",
    "        # 'Precision':   np.round(Precision,   digit),\n",
    "        'Sensitivity': np.round(Recall,      digit),\n",
    "        'Specificity': np.round(Specificity, digit),\n",
    "        'Threshold':   np.round(cutoff,      digit),\n",
    "        'F1':          np.round(F1,          digit),\n",
    "    }\n",
    "\n",
    "\n",
    "# Set LightGBM parameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_error',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.03,\n",
    "    'num_leaves': 51,\n",
    "    'feature_fraction': 0.62,\n",
    "    'bagging_fraction': 1,\n",
    "    'bagging_freq': 6,\n",
    "    'verbose': 0,\n",
    "    'max_depth': 20,  # Set maximum tree depth\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b650b4-9bdb-4c40-a5e3-69a8acbf6cfd",
   "metadata": {},
   "source": [
    "### TCR-pMHC train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f54830-2b0a-4a93-807f-6bbf641a4490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(df, data_1, data_2, params):\n",
    "\n",
    "    labels = list(df.binder)\n",
    "    labels = np.array(labels)\n",
    "    labels = labels.reshape(-1, 1)\n",
    "\n",
    "    CDR3_flattened = data_2.reshape(data_2.shape[0], -1)\n",
    "    features = np.concatenate((data_1, CDR3_flattened), axis=1)\n",
    "\n",
    "    # Split dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=342)\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    # test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "    # Train LightGBM model\n",
    "    bst = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=400,\n",
    "        # valid_sets=[test_data],\n",
    "        # callbacks=[lgb.early_stopping(stopping_rounds=30)]\n",
    "    )\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    test_predictions = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "    test_predicted_labels = np.where(test_predictions > 0.5, 1, 0)\n",
    "\n",
    "    print(compute_metrics1(y_test, test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad930469-620a-49e0-a2c3-a4f8207bbc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Load Data and Train Model ###################\n",
    "\n",
    "data_1 = torch.load(\"PMHC_encodeing/PMHC_result/pmhc_large_data_encoding.pt\")\n",
    "data_2 = torch.load(\"TCR_encodeing/TCR_result/TCR_encoding/TCR_large_encoding.pt\")\n",
    "df = load_hla_dataframe('PMHC_encodeing/data/large_data.csv')\n",
    "\n",
    "train_and_test(df, data_1, data_2, params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9365006a-e901-4982-a7fc-e9cb24c6c0eb",
   "metadata": {},
   "source": [
    "## Generalization ability test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921b6c8d-3f2e-442c-88cc-3168fb87318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Define Training and Testing Functions ####################\n",
    "\n",
    "def train(df, data_1, data_2, params):\n",
    "    labels = list(df.binder)\n",
    "    # print(len(labels))\n",
    "    labels = np.array(labels)\n",
    "    labels = labels.reshape(-1, 1)\n",
    "\n",
    "    # Flatten and merge data\n",
    "    CDR3_flattened = data_2.reshape(data_2.shape[0], -1)\n",
    "    features = np.concatenate((data_1, CDR3_flattened), axis=1)\n",
    "\n",
    "    # Random under-sampling of data\n",
    "    nm = RandomUnderSampler(random_state=42)\n",
    "    X_res, y_res = nm.fit_resample(features, labels)\n",
    "\n",
    "    # Create LightGBM dataset\n",
    "    train_data = lgb.Dataset(X_res, label=y_res)\n",
    "\n",
    "    bst = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=400,\n",
    "        # valid_sets=[test_data],\n",
    "    )\n",
    "    print('trian done !!!!')\n",
    "    \n",
    "    return bst\n",
    "\n",
    "def test(df_test, data_test_1, data_test_2, bst):\n",
    "    labels_test = list(df_test.binder)\n",
    "    # print(len(labels_test))\n",
    "    labels_test = np.array(labels_test)\n",
    "    labels_test = labels_test.reshape(-1, 1)\n",
    "\n",
    "    # Flatten and merge test data\n",
    "    test_CDR3_flattened = data_test_2.reshape(data_test_2.shape[0], -1)\n",
    "    test_features = np.concatenate((data_test_1, test_CDR3_flattened), axis=1)\n",
    "\n",
    "    # Make predictions using the test set\n",
    "    test_predictions = bst.predict(test_features, num_iteration=bst.best_iteration)\n",
    "    test_predicted_labels = np.where(test_predictions > 0.5, 1, 0)\n",
    "\n",
    "    # Compute and print metrics\n",
    "    print(compute_metrics1(labels_test, test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78c1490-f3f8-44ca-a9cf-fb107824bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Load Training Dataset and Train Model ######################\n",
    "\n",
    "numpy_array = np.load('PMHC_encodeing/PMHC_result/epiTCR_data_encoding/pmhc_train_encoding.npy')\n",
    "data_1 = torch.from_numpy(numpy_array)\n",
    "data_2 = torch.load(\"TCR_encodeing/TCR_result/TCR_encoding/TCR_train.pt\")\n",
    "df = load_hla_dataframe('PMHC_encodeing/data/epiTCR_data/train.csv')\n",
    "\n",
    "bst = train(df, data_1, data_2, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763cbddf-7504-4f83-bb2f-961d492a6e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Load Testing Dataset and Test Model ################\n",
    "\n",
    "data_test_1 = torch.load(\"PMHC_encodeing/PMHC_result/epiTCR_data_encoding/pmhc_test01_encoding.pt\")\n",
    "data_test_2 = torch.load(\"TCR_encodeing/TCR_result/TCR_encoding/TCR_test01.pt\")\n",
    "df_test = load_hla_dataframe('PMHC_encodeing/data/epiTCR_data/test01.csv')\n",
    "\n",
    "test(df_test, data_test_1, data_test_2, bst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abff6882-a413-4fff-9afd-9ab0da9e7b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b733f8-af6b-4aa3-8c35-8bc7d216bd45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
